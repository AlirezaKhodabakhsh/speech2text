{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python\\Anaconda3\\envs\\speech2text\\lib\\site-packages\\transformers\\feature_extraction_utils.py:168: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tensor = as_tensor(value)\n",
      "C:\\python\\Anaconda3\\envs\\speech2text\\lib\\site-packages\\transformers\\feature_extraction_utils.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  value = np.array(value)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32mC:\\python\\Anaconda3\\envs\\speech2text\\lib\\site-packages\\transformers\\feature_extraction_utils.py:168\u001B[0m, in \u001B[0;36mBatchFeature.convert_to_tensors\u001B[1;34m(self, tensor_type)\u001B[0m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_tensor(value):\n\u001B[1;32m--> 168\u001B[0m     tensor \u001B[38;5;241m=\u001B[39m \u001B[43mas_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    170\u001B[0m     \u001B[38;5;28mself\u001B[39m[key] \u001B[38;5;241m=\u001B[39m tensor\n",
      "File \u001B[1;32mC:\\python\\Anaconda3\\envs\\speech2text\\lib\\site-packages\\transformers\\feature_extraction_utils.py:150\u001B[0m, in \u001B[0;36mBatchFeature.convert_to_tensors.<locals>.as_tensor\u001B[1;34m(value)\u001B[0m\n\u001B[0;32m    149\u001B[0m     value \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(value)\n\u001B[1;32m--> 150\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 66>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     61\u001B[0m         x_train\u001B[38;5;241m.\u001B[39mappend(speech)\n\u001B[0;32m     62\u001B[0m         y_train\u001B[38;5;241m.\u001B[39mappend(drug)\n\u001B[1;32m---> 66\u001B[0m input_values \u001B[38;5;241m=\u001B[39m \u001B[43mprocessor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msampling_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39minput_values\n\u001B[0;32m     68\u001B[0m \u001B[38;5;66;03m# encode labels\u001B[39;00m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m processor\u001B[38;5;241m.\u001B[39mas_target_processor():\n",
      "File \u001B[1;32mC:\\python\\Anaconda3\\envs\\speech2text\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:73\u001B[0m, in \u001B[0;36mWav2Vec2Processor.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;124;03m    When used in normal mode, this method forwards all its arguments to Wav2Vec2FeatureExtractor's\u001B[39;00m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;124;03m    [`~Wav2Vec2FeatureExtractor.__call__`] and returns its output. If used in the context\u001B[39;00m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;124;03m    [`~Wav2Vec2Processor.as_target_processor`] this method forwards all its arguments to PreTrainedTokenizer's\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;124;03m    [`~PreTrainedTokenizer.__call__`]. Please refer to the docstring of the above two methods for more information.\u001B[39;00m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_processor(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\python\\Anaconda3\\envs\\speech2text\\lib\\site-packages\\transformers\\models\\wav2vec2\\feature_extraction_wav2vec2.py:234\u001B[0m, in \u001B[0;36mWav2Vec2FeatureExtractor.__call__\u001B[1;34m(self, raw_speech, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors, sampling_rate, **kwargs)\u001B[0m\n\u001B[0;32m    229\u001B[0m     padded_inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_values\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mzero_mean_unit_var_norm(\n\u001B[0;32m    230\u001B[0m         padded_inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_values\u001B[39m\u001B[38;5;124m\"\u001B[39m], attention_mask\u001B[38;5;241m=\u001B[39mattention_mask, padding_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_value\n\u001B[0;32m    231\u001B[0m     )\n\u001B[0;32m    233\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_tensors \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 234\u001B[0m     padded_inputs \u001B[38;5;241m=\u001B[39m \u001B[43mpadded_inputs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_to_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    236\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m padded_inputs\n",
      "File \u001B[1;32mC:\\python\\Anaconda3\\envs\\speech2text\\lib\\site-packages\\transformers\\feature_extraction_utils.py:174\u001B[0m, in \u001B[0;36mBatchFeature.convert_to_tensors\u001B[1;34m(self, tensor_type)\u001B[0m\n\u001B[0;32m    172\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverflowing_values\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    173\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to create tensor returning overflowing values of different lengths. \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 174\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    175\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to create tensor, you should probably activate padding \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    176\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwith \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpadding=True\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m to have batched tensors with the same length.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    177\u001B[0m         )\n\u001B[0;32m    179\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[1;31mValueError\u001B[0m: Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2769\n"
     ]
    }
   ],
   "source": [
    "###ensemble preprocess\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, HubertForCTC\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch\n",
    "import librosa\n",
    "import pickle\n",
    "import pydub\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class prepare_data():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "        self.model = HubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "\n",
    "\n",
    "    def read(f, normalized=False):\n",
    "        \"\"\"MP3 to numpy array\"\"\"\n",
    "        a = pydub.AudioSegment.from_mp3(f)\n",
    "        a = a.set_frame_rate(16000)\n",
    "        y = np.array(a.get_array_of_samples())\n",
    "        if a.channels == 2:\n",
    "            y = y.reshape((-1, 2))\n",
    "        if normalized:\n",
    "            return a.frame_rate, np.float32(y) / 2**15\n",
    "        else:\n",
    "            return a.frame_rate, y\n",
    "\n",
    "\n",
    "    def prep_data(self, path):\n",
    "\n",
    "        with open(path, 'rb') as output:\n",
    "          data = pickle.load(output)\n",
    "\n",
    "        l = 0\n",
    "        x_train = []\n",
    "        y_train = []\n",
    "        bug = {}\n",
    "        # load audio and train\n",
    "        for drug in data.keys():\n",
    "\n",
    "            l += 1\n",
    "            for j in range(len(data[drug]['brand_audio'])):\n",
    "\n",
    "                file = data[drug]['brand_audio'][j]['path']\n",
    "                path = 'speech2text' + file[1:]\n",
    "\n",
    "                if path[-3:] == 'wav':\n",
    "\n",
    "                  try:\n",
    "                      speech, _ = librosa.load(path, sr=16000)\n",
    "\n",
    "                  except:\n",
    "                      bug[drug] = j\n",
    "                      continue\n",
    "\n",
    "                elif path[-3:] == 'mp3':\n",
    "\n",
    "                  try:\n",
    "                      _, speech = self.read(path, normalized=True)\n",
    "                  except:\n",
    "                      bug[drug] = j\n",
    "                      continue\n",
    "\n",
    "                else:\n",
    "                    bug[drug] = j\n",
    "                    continue\n",
    "\n",
    "                input_values = self.processor(speech, sampling_rate=_, return_tensors=\"pt\")\n",
    "                target_transcription = drug.upper()\n",
    "\n",
    "            # encode labels\n",
    "                with self.processor.as_target_processor():\n",
    "                  input_values['labels'] = self.processor(target_transcription, return_tensors=\"pt\").input_ids\n",
    "\n",
    "                x_train.append(input_values)\n",
    "\n",
    "        return x_train, bug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}